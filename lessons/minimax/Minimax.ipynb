{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Minimax\n",
    "The minimax algorythm can be very useful when deciding next moves, though it is only a loose fit in the category of \"machine learning\". This is because minimax is not trained and does not learn. Instead, it acts more like we traditionally assume computers would work: it computes every possibility and chooses the best one. \"Best\" is a little to vague for us, so let's define it a bit more. Minimax stands for maximizing the minimum gain. This means the algorythm will always \"play it safe\" and choose the least risky option. So if the two choices were: \"maybe lose 3 points or maybe gain 10\" and \"maybe lose 2 points or maybe gain 1\" it will allways take the latter. Now this may not seem particularly helpful, but there are many situations where it works very well. One of those is Tic-Tac-Toe, which you can look at in the \"examples\" folder. First, here is a great video explaining how the algorythm works in detail. Feel free to pause and re-watch as often as needed to understand."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "[![Minimax - Sebastian Lague](https://img.youtube.com/vi/l-hh51ncgDI/0.jpg)](https://www.youtube.com/watch?v=l-hh51ncgDI)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here is the algorythm from the video written in python (note `state` functions are not implemented):"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def minimax(state, depth, alpha, beta, maximizing_player):\n",
    "    if depth == 0 or state.is_game_over():\n",
    "        return state.evaluation()\n",
    "\n",
    "    if maximizing_player:\n",
    "        max_eval = -np.Infinity\n",
    "        for next_state in state.next_possible_states():\n",
    "            evaluation = minimax(next_state, depth - 1, alpha, beta, False)\n",
    "            max_eval = max(max_eval, evaluation)\n",
    "            alpha = max(alpha, evaluation)\n",
    "            if beta <= alpha:\n",
    "                break\n",
    "        return max_eval\n",
    "\n",
    "    else:\n",
    "        min_eval = np.Infinity\n",
    "        for next_state in state.next_possible_states():\n",
    "            evaluation = minimax(next_state, depth - 1, alpha, beta, True)\n",
    "            min_eval = min(min_eval, evaluation)\n",
    "            alpha = min(beta, evaluation)\n",
    "            if beta <= alpha:\n",
    "                break\n",
    "        return min_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Depending on what you are utilizing the algorythm for, it will likely need some tweaking. For basic Tic-Tac-Toe, a game with no intermediate values, it will look something like this:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def minimax(state, depth, alpha, beta, maximizing_player):\n",
    "    #returns either 1, -1, or 0 for no winner\n",
    "    winner = state.winner()\n",
    "    #returns if max depth reached, someone has won, or there are no moves left\n",
    "    if depth == 0 or winner != 0 or len(state.next_possible_states()) == 0:\n",
    "        return winner\n",
    "\n",
    "    if maximizing_player:\n",
    "        max_eval = -np.Infinity\n",
    "        for next_state in state.next_possible_states():\n",
    "            evaluation = minimax(next_state, depth - 1, alpha, beta, False)\n",
    "            max_eval = max(max_eval, evaluation)\n",
    "            alpha = max(alpha, evaluation)\n",
    "            if beta <= alpha:\n",
    "                break\n",
    "        return max_eval\n",
    "\n",
    "    else:\n",
    "        min_eval = np.Infinity\n",
    "        for next_state in state.next_possible_states():\n",
    "            evaluation = minimax(next_state, depth - 1, alpha, beta, True)\n",
    "            min_eval = min(min_eval, evaluation)\n",
    "            alpha = min(beta, evaluation)\n",
    "            if beta <= alpha:\n",
    "                break\n",
    "        return min_eval"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Currently, this version of minimax treats all wins equally. However, it would be better if it preferred quicker wins. As such, let's add a decay that lowers the value every move, causing it to prefer quicker wins. This also ends up making it lose slower, allowing more opportunities for the opposing player to make an error."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "decay_rate = .9\n",
    "def minimax(state, depth, alpha, beta, maximizing_player):\n",
    "    #returns either 1, -1, or 0 for no winner\n",
    "    winner = state.winner()\n",
    "    #returns if max depth reached, someone has won, or there are no moves left\n",
    "    if depth == 0 or winner != 0 or len(state.next_possible_states()) == 0:\n",
    "        return winner\n",
    "\n",
    "    if maximizing_player:\n",
    "        max_eval = -np.Infinity\n",
    "        for next_state in state.next_possible_states():\n",
    "            #loses 10% of it's value for every move required\n",
    "            evaluation = decay_rate * minimax(next_state, depth - 1, alpha, beta, False)\n",
    "            max_eval = max(max_eval, evaluation)\n",
    "            alpha = max(alpha, evaluation)\n",
    "            if beta <= alpha:\n",
    "                break\n",
    "        return max_eval\n",
    "\n",
    "    else:\n",
    "        min_eval = np.Infinity\n",
    "        for next_state in state.next_possible_states():\n",
    "            #loses 10% of it's value for every move required\n",
    "            evaluation = decay_rate * minimax(next_state, depth - 1, alpha, beta, True)\n",
    "            min_eval = min(min_eval, evaluation)\n",
    "            alpha = min(beta, evaluation)\n",
    "            if beta <= alpha:\n",
    "                break\n",
    "        return min_eval"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The above code is still not that great, as it assumes the opponent will play perfectly. Therefore, will not consider plays that might cause a win as it assumes the opponent will see them. In Tic-Tac-Toe, many of the possible moves will lead to a tie and therefore have the same apparent value to minimax. We can make this smarter by including a bias, meaning that even if all options are equal in perfect play, the one with the most possibilities for a win gets chosen. We can do this by also adding a small amount to the output which will only matter when the main minimax values are equal."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "decay_rate = .9\n",
    "bias_multiplier = .01\n",
    "def minimax(state, depth, alpha, beta, maximizing_player):\n",
    "    #returns either 1, -1, or 0 for no winner\n",
    "    winner = state.winner()\n",
    "    #initialize bias to 0\n",
    "    bias = 0\n",
    "    #returns if max depth reached, someone has won, or there are no moves left\n",
    "    if depth == 0 or winner != 0 or len(state.next_possible_states()) == 0:\n",
    "        if winner!= 0:\n",
    "            bias += winner * bias_multiplier\n",
    "        return winner, bias\n",
    "\n",
    "    if maximizing_player:\n",
    "        max_eval = -np.Infinity\n",
    "        for next_state in state.next_possible_states():\n",
    "            evaluation, delta_bias = minimax(next_state, depth - 1, alpha, beta, False)\n",
    "            #loses 10% of it's value for every move required\n",
    "            evaluation *= decay_rate\n",
    "            #update this moves bias based on next moves' bias\n",
    "            bias += delta_bias\n",
    "            max_eval = max(max_eval, evaluation)\n",
    "            alpha = max(alpha, evaluation)\n",
    "            if beta <= alpha:\n",
    "                break\n",
    "        return max_eval, bias\n",
    "\n",
    "    else:\n",
    "        min_eval = np.Infinity\n",
    "        for next_state in state.next_possible_states():\n",
    "            evaluation, delta_bias = minimax(next_state, depth - 1, alpha, beta, True)\n",
    "            #loses 10% of it's value for every move required\n",
    "            evaluation *= decay_rate\n",
    "            #update this moves bias based on next moves' bias\n",
    "            bias += delta_bias\n",
    "            min_eval = min(min_eval, evaluation)\n",
    "            alpha = min(beta, evaluation)\n",
    "            if beta <= alpha:\n",
    "                break\n",
    "        return min_eval, bias"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now have a algorythm which will work well with Tic-Tac-Toe like games. While the original algorythm is best suited for games with live scores, it can be easily modified to suit your needs. Hopefully this gave you some ideas about how to use minimax in your own projects, and how to approach modifying it to better suit your needs. Check out the \"examples\" directory for a simple minimax Tic-Tac-Toe implementation as well as a more complex one using it in conjunction with reinforcement learning."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
